import streamlit as st
import analyzer
import pandas as pd
import time
import tiktoken

NAME_MAP = {
    "ì˜¤ìŠ¹í™˜": "Seunghwan Oh",
    "ë°•ìœ ë¯¸": "Yumi Park",
    "í˜„ì§„": "Hyunjin",
    "ë°•ì›ì¤€": "Wonjoon Park",
    "ë°•ë²•ì¤€": "Beobjun Park",
    "ê¹€ì¬ìš©": "Jaeyong Kim",
    "ê¹€ì§„ê´€": "Jingwan Kim",
    "ì–‘ì„ì¤€": "Seokjun Yang",
    "JD": "JD"
}
try:
    from hangul_romanize import Transliter
    from hangul_romanize.rule import academic
    transliter = Transliter(academic)
    def to_eng_name(name):
        if name in NAME_MAP:
            return NAME_MAP[name]
        return transliter.translit(name)
except ImportError:
    def to_eng_name(name):
        return NAME_MAP.get(name, name)

NOTICE = (
    "â€» ë³¸ ê²°ê³¼ëŠ” í…ŒìŠ¤íŠ¸/í”„ë¡œí† íƒ€ì… ë²„ì „ì´ë©°, ì¸ë¬¼ í‰ê°€ëŠ” ì•„ë‹Œ 'í–‰ë™ ê¸°ë°˜ ë°ì´í„°' ê¸°ì¤€ ì„ì‹œ ë¶„ì„ì…ë‹ˆë‹¤. "
    "ì‹¤ì œ ì¸ë¬¼ í‰ê°€ë¡œ ì˜¤ìš©ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²°ê³¼ í™œìš© ì „ ì¶”ê°€ ê·¼ê±° ë° ë§¥ë½ ì„¤ëª…ì„ ì°¸ê³ í•˜ì„¸ìš”."
)
COPYRIGHT = "Â© 2025 Sunghwan Oh. All rights reserved. This MirrorOrg MVP is a test/experimental project. Not for commercial use."

TEXTS = {
    "page_title": {"ko": "MirrorOrg ë‹¨ê³„ë³„ MVP", "en": "MirrorOrg Stepwise MVP"},
    "main_title": {"ko": "ğŸª MirrorOrg ë‹¨ê³„ë³„ íŒ€ ë¶„ì„", "en": "ğŸª MirrorOrg Stepwise Team Analysis"},
    "main_description": {
        "ko": "â‘  íŒŒì¼ ì—…ë¡œë“œ â†’ â‘¡ ì±•í„°ë³„ ë¶„ì„ ì‹¤í–‰ â†’ â‘¢ ê²°ê³¼ í™•ì¸ì˜ ìˆœì„œë¡œ ì•ˆì „í•˜ê³  ì§ê´€ì ì¸ íŒ€ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "en": "Upload file â†’ Run each chapter â†’ View result. This safe, clear flow ensures robust team analysis."
    },
    "sidebar_header": {"ko": "ì„¤ì •", "en": "Settings"},
    "language_selector": {"ko": "ì–¸ì–´", "en": "Language"},
    "upload_header": {"ko": "1ï¸âƒ£ ì±„íŒ… ê¸°ë¡ ì—…ë¡œë“œ", "en": "1ï¸âƒ£ Upload Chat History"},
    "upload_info": {
        "ko": "íŒ€ ì±„íŒ… ê¸°ë¡ì„ .txt íŒŒì¼ë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”. ì—…ë¡œë“œ ì „ê¹Œì§€ ì•„ë˜ ë‹¨ê³„ëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.",
        "en": "Upload your team chat history as a .txt file. Steps below are disabled until upload."
    },
    "file_uploader_label": {"ko": "ë¶„ì„í•  .txt íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", "en": "Choose a .txt file to analyze."},
    "chapter_header": {"ko": "2ï¸âƒ£ ì±•í„°ë³„ ë¶„ì„ ì‹¤í–‰", "en": "2ï¸âƒ£ Run Each Analysis Chapter"},
    "chapter1_btn": {"ko": "ì±•í„° 1: ì¢…í•© ë³´ê³ ì„œ", "en": "Chapter 1: Comprehensive Report"},
    "chapter2_btn": {"ko": "ì±•í„° 2: í”¼ë¡œë„ ê³¡ì„ ", "en": "Chapter 2: Fatigue Trajectory"},
    "chapter3_btn": {"ko": "ì±•í„° 3: ê´€ê³„ ë„¤íŠ¸ì›Œí¬", "en": "Chapter 3: Relationship Network"},
    "analysis_complete": {"ko": "âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", "en": "âœ… Analysis complete!"},
    "results_header": {"ko": "3ï¸âƒ£ ê²°ê³¼ í™•ì¸", "en": "3ï¸âƒ£ View Results"},
    "fatigue_title": {"ko": "ì±•í„° 2 ê²°ê³¼: í”¼ë¡œë„ ê³¡ì„ ", "en": "Chapter 2 Result: Fatigue Trajectory"},
    "network_title": {"ko": "ì±•í„° 3 ê²°ê³¼: ê´€ê³„ ë„¤íŠ¸ì›Œí¬", "en": "Chapter 3 Result: Relationship Network"},
    "no_fatigue_data": {"ko": "í”¼ë¡œë„ ê³¡ì„  ë°ì´í„°ë¥¼ ì‹œê°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "en": "Fatigue trajectory data could not be visualized."},
    "no_network_data": {"ko": "ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ë¥¼ ì‹œê°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "en": "Network data could not be visualized."},
    "report_title": {"ko": "ì±•í„° 1 ê²°ê³¼: ì¢…í•© ë³´ê³ ì„œ", "en": "Chapter 1 Result: Comprehensive Report"},
    "raw_llm": {"ko": "LLM ì›ë³¸ ì‘ë‹µ(raw)", "en": "LLM Raw Response"},
}

st.set_page_config(page_title=TEXTS["page_title"]["en"], page_icon="ğŸ¤–", layout="wide")
if 'lang' not in st.session_state:
    st.session_state.lang = 'ko'

with st.sidebar:
    st.header(TEXTS["sidebar_header"][st.session_state.lang])
    lang_choice = st.selectbox(
        label=f'{TEXTS["language_selector"]["en"]} / {TEXTS["language_selector"]["ko"]}',
        options=['í•œêµ­ì–´', 'English'],
        index=0 if st.session_state.lang == 'ko' else 1
    )
    st.session_state.lang = 'ko' if lang_choice == 'í•œêµ­ì–´' else 'en'
    st.markdown("---")
    st.caption(NOTICE)
    st.markdown("---")
    st.caption(COPYRIGHT)

lang = st.session_state.lang

st.title(TEXTS["main_title"][lang])
st.markdown(TEXTS["main_description"][lang])

st.header(TEXTS["upload_header"][lang])
st.info(TEXTS["upload_info"][lang])
uploaded_file = st.file_uploader(TEXTS["file_uploader_label"][lang], type="txt")

if not uploaded_file:
    st.stop()

file_content = uploaded_file.getvalue().decode("utf-8")
st.success(f"'{uploaded_file.name}' íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

MAX_TOKENS = 14000
MAX_LINES = 2000

def count_tokens(text, model="gpt-3.5-turbo"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def get_short_content(file_content):
    lines = file_content.splitlines()
    if len(lines) > MAX_LINES:
        st.warning(f"ë¶„ì„ ë°ì´í„°ê°€ ë§ì•„ ìµœì‹  {MAX_LINES}ì¤„(ì•½ 2ê°œì›”ì¹˜)ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    short_text = "\n".join(lines[-MAX_LINES:])
    while count_tokens(short_text) > MAX_TOKENS and len(lines) > 50:
        lines = lines[-(len(lines)//2):]
        short_text = "\n".join(lines)
    if count_tokens(short_text) > MAX_TOKENS:
        st.warning("íŒŒì¼ì´ ë„ˆë¬´ ì»¤ì„œ ë” ì‘ê²Œ ì˜ë¼ ìƒ˜í”Œ ë¶„ì„í•©ë‹ˆë‹¤.")
    return short_text

st.header(TEXTS["chapter_header"][lang])
col1, col2, col3 = st.columns(3)

with col1:
    if st.button(TEXTS["chapter1_btn"][lang], use_container_width=True):
        with st.spinner("ë³´ê³ ì„œ ìƒì„± ì¤‘... (ìµœëŒ€ 1ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŒ)"):
            short_content = get_short_content(file_content)
            start = time.time()
            report = analyzer.generate_report(short_content, lang=lang, sample_mode=True)
            elapsed = time.time() - start
            if elapsed > 60:
                st.warning("ë¶„ì„ì´ ì˜¤ë˜ ê±¸ë ¤ ìµœì‹  2000ì¤„ ìƒ˜í”Œ ë¶„ì„ìœ¼ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
                short_content = get_short_content(file_content)
                report = analyzer.generate_report(short_content, lang=lang, sample_mode=True)
                st.info("ìƒ˜í”Œ(ìµœê·¼ 2000ì¤„) ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.")
            st.session_state.report = report
        st.toast(TEXTS["analysis_complete"][lang], icon="âœ…")

with col2:
    if st.button(TEXTS["chapter2_btn"][lang], use_container_width=True):
        with st.spinner("í”¼ë¡œë„ ë¶„ì„ ì¤‘..."):
            st.session_state.fatigue_data = analyzer.analyze_fatigue_json(get_short_content(file_content), lang=lang)
        st.toast(TEXTS["analysis_complete"][lang], icon="âœ…")

with col3:
    if st.button(TEXTS["chapter3_btn"][lang], use_container_width=True):
        with st.spinner("ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘..."):
            st.session_state.network_data = analyzer.analyze_network_json(get_short_content(file_content), lang=lang)
        st.toast(TEXTS["analysis_complete"][lang], icon="âœ…")

st.header(TEXTS["results_header"][lang])

if st.session_state.get('report'):
    st.subheader(TEXTS["report_title"][lang])
    st.markdown(st.session_state.report, unsafe_allow_html=True)
    st.caption(NOTICE)
    st.divider()

if st.session_state.get('fatigue_data'):
    st.subheader(TEXTS["fatigue_title"][lang])
    fatigue_data = st.session_state.get('fatigue_data')
    if fatigue_data and isinstance(fatigue_data, list):
        try:
            lines = []
            for item in fatigue_data:
                eng_name = to_eng_name(item["name"])
                for d in item["fatigue_timeline"]:
                    lines.append({"name": eng_name, "date": d["date"], "score": d["score"]})
            if not lines:
                st.warning("ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŒ")
            else:
                df = pd.DataFrame(lines)
                chart_data = df.pivot(index="date", columns="name", values="score")
                st.line_chart(chart_data)
                st.caption(NOTICE)
        except Exception as e:
            st.error(f"{TEXTS['no_fatigue_data'][lang]}: {e}")
    elif fatigue_data and "raw_response" in fatigue_data:
        st.warning(TEXTS["raw_llm"][lang] + ":")
        st.code(fatigue_data["raw_response"])
        st.info(TEXTS["no_fatigue_data"][lang])
    else:
        st.info(TEXTS["no_fatigue_data"][lang])

if st.session_state.get('network_data'):
    st.subheader(TEXTS["network_title"][lang])
    network_data

