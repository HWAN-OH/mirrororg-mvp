import streamlit as st
import analyzer
import pandas as pd
import time
import tiktoken
import networkx as nx
import matplotlib.pyplot as plt

# (ì´ë¦„ ë³€í™˜ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜, í•„ìš” ì—†ìœ¼ë©´ ì‚­ì œ)
NAME_MAP = {
    "ì˜¤ìŠ¹í™˜": "Seunghwan Oh",
    "ë°•ìœ ë¯¸": "Yumi Park",
    "í˜„ì§„": "Hyunjin",
    "ë°•ì›ì¤€": "Wonjoon Park",
    "ë°•ë²•ì¤€": "Beobjun Park",
    "ê¹€ì¬ìš©": "Jaeyong Kim",
    "ê¹€ì§„ê´€": "Jingwan Kim",
    "ì–‘ì„ì¤€": "Seokjun Yang",
    "JD": "JD"
}
def to_eng_name(name):
    return NAME_MAP.get(name, name)

NOTICE_KO = "â€» ë³¸ ê²°ê³¼ëŠ” í…ŒìŠ¤íŠ¸/í”„ë¡œí† íƒ€ì… ë²„ì „ì´ë©°, ì¸ë¬¼ í‰ê°€ëŠ” ì•„ë‹Œ 'í–‰ë™ ê¸°ë°˜ ë°ì´í„°' ê¸°ì¤€ ì„ì‹œ ë¶„ì„ì…ë‹ˆë‹¤. ì‹¤ì œ ì¸ë¬¼ í‰ê°€ë¡œ ì˜¤ìš©ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²°ê³¼ í™œìš© ì „ ì¶”ê°€ ê·¼ê±° ë° ë§¥ë½ ì„¤ëª…ì„ ì°¸ê³ í•˜ì„¸ìš”."
NOTICE_EN = "â€» This result is a test/prototype version, based on behavioral data, not a personality judgment. Cannot be used as a real person evaluation. Please refer to additional context before applying the results."
COPYRIGHT = "Â© 2025 Sunghwan Oh. All rights reserved. This MirrorOrg MVP is a test/experimental project. Not for commercial use."

TEXTS = {
    "page_title": {"ko": "MirrorOrg ë„¤íŠ¸ì›Œí¬ ë¶„ì„ MVP", "en": "MirrorOrg Network Analysis MVP"},
    "main_title": {"ko": "ğŸª MirrorOrg ì¡°ì§ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„", "en": "ğŸª MirrorOrg Organizational Network Analysis"},
    "main_description": {
        "ko": "â‘  íŒŒì¼ ì—…ë¡œë“œ â†’ â‘¡ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤í–‰ â†’ â‘¢ ê²°ê³¼ í™•ì¸ì˜ ì•ˆì „í•˜ê³  ì§ê´€ì  íë¦„ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "en": "Upload file â†’ Run network analysis â†’ View result. Simple and safe workflow."
    },
    "sidebar_header": {"ko": "ì„¤ì •", "en": "Settings"},
    "language_selector": {"ko": "ì–¸ì–´", "en": "Language"},
    "upload_header": {"ko": "1ï¸âƒ£ ì±„íŒ… ê¸°ë¡ ì—…ë¡œë“œ", "en": "1ï¸âƒ£ Upload Chat History"},
    "upload_info": {
        "ko": "íŒ€ ì±„íŒ… ê¸°ë¡ì„ .txt íŒŒì¼ë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”. ì—…ë¡œë“œ ì „ê¹Œì§€ ì•„ë˜ ë‹¨ê³„ëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.",
        "en": "Upload your team chat history as a .txt file. Steps below are disabled until upload."
    },
    "file_uploader_label": {"ko": "ë¶„ì„í•  .txt íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", "en": "Choose a .txt file to analyze."},
    "chapter_header": {"ko": "2ï¸âƒ£ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤í–‰", "en": "2ï¸âƒ£ Run Relationship Network Analysis"},
    "chapter3_btn": {"ko": "ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„", "en": "Run Network Analysis"},
    "analysis_complete": {"ko": "âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", "en": "âœ… Analysis complete!"},
    "results_header": {"ko": "3ï¸âƒ£ ê²°ê³¼ í™•ì¸", "en": "3ï¸âƒ£ View Results"},
    "network_title": {"ko": "ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ê²°ê³¼", "en": "Network Analysis Result"},
    "no_network_data": {"ko": "ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ë¥¼ ì‹œê°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "en": "Network data could not be visualized."},
    "raw_llm": {"ko": "LLM ì›ë³¸ ì‘ë‹µ(raw)", "en": "LLM Raw Response"},
}

st.set_page_config(page_title=TEXTS["page_title"]["en"], page_icon="ğŸ¤–", layout="wide")
if 'lang' not in st.session_state:
    st.session_state.lang = 'ko'

with st.sidebar:
    st.header(f"{TEXTS['sidebar_header']['ko']} / {TEXTS['sidebar_header']['en']}")
    lang_choice = st.selectbox(
        label=f"{TEXTS['language_selector']['ko']} / {TEXTS['language_selector']['en']}",
        options=['í•œêµ­ì–´', 'English'],
        index=0 if st.session_state.lang == 'ko' else 1
    )
    st.session_state.lang = 'ko' if lang_choice == 'í•œêµ­ì–´' else 'en'
    st.markdown("---")
    st.caption(f"{NOTICE_KO}\n\n{NOTICE_EN}")
    st.markdown("---")
    st.caption(COPYRIGHT)

lang = st.session_state.lang

st.title(f"{TEXTS['main_title']['ko']} / {TEXTS['main_title']['en']}")
st.markdown(f"{TEXTS['main_description']['ko']}<br/>{TEXTS['main_description']['en']}", unsafe_allow_html=True)

st.header(f"{TEXTS['upload_header'][lang]}")
st.info(TEXTS["upload_info"][lang])
uploaded_file = st.file_uploader(TEXTS["file_uploader_label"][lang], type="txt")

if not uploaded_file:
    st.stop()

file_content = uploaded_file.getvalue().decode("utf-8")
st.success(f"'{uploaded_file.name}' íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

MAX_TOKENS = 14000
MAX_LINES = 2000

def count_tokens(text, model="gpt-3.5-turbo"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def get_short_content(file_content):
    lines = file_content.splitlines()
    if len(lines) > MAX_LINES:
        st.warning(f"ë¶„ì„ ë°ì´í„°ê°€ ë§ì•„ ìµœì‹  {MAX_LINES}ì¤„(ì•½ 2ê°œì›”ì¹˜)ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    short_text = "\n".join(lines[-MAX_LINES:])
    while count_tokens(short_text) > MAX_TOKENS and len(lines) > 50:
        lines = lines[-(len(lines)//2):]
        short_text = "\n".join(lines)
    if count_tokens(short_text) > MAX_TOKENS:
        st.warning("íŒŒì¼ì´ ë„ˆë¬´ ì»¤ì„œ ë” ì‘ê²Œ ì˜ë¼ ìƒ˜í”Œ ë¶„ì„í•©ë‹ˆë‹¤.")
    return short_text

st.header(TEXTS["chapter_header"][lang])
if st.button(f"{TEXTS['chapter3_btn']['ko']} / {TEXTS['chapter3_btn']['en']}", use_container_width=True):
    with st.spinner("ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘... / Running network analysis..."):
        st.session_state.network_data = analyzer.analyze_network_json(get_short_content(file_content), lang=lang)
    st.toast(TEXTS["analysis_complete"][lang], icon="âœ…")

st.header(TEXTS["results_header"][lang])

if st.session_state.get('network_data'):
    st.subheader(f"{TEXTS['network_title']['ko']} / {TEXTS['network_title']['en']}")
    network_data = st.session_state.get('network_data')
    if network_data and isinstance(network_data, list):
        try:
            # 1. í‘œë¡œ ë³´ê¸° (ì˜ë¬¸ëª… ìë™ ë³€í™˜)
            lines = []
            for rel in network_data:
                src = to_eng_name(rel.get("source", ""))
                tgt = to_eng_name(rel.get("target", ""))
                strength = rel.get("strength")
                typ = rel.get("type")
                lines.append({
                    "Source": src,
                    "Target": tgt,
                    "Strength": strength,
                    "Type": typ
                })
            df = pd.DataFrame(lines)
            st.markdown("**ğŸ”— ê´€ê³„ ë„¤íŠ¸ì›Œí¬ í‘œ / Relationship Table**")
            st.dataframe(df)

            # 2. ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ì‹œê°í™”
            st.markdown("**ğŸŒ ë„¤íŠ¸ì›Œí¬ ë‹¤ì´ì–´ê·¸ë¨ / Network Diagram**")
            G = nx.DiGraph()
            for rel in lines:
                G.add_edge(rel["Source"], rel["Target"], weight=rel["Strength"], label=rel["Type"])
            pos = nx.spring_layout(G, seed=42, k=0.7)
            plt.figure(figsize=(6, 4))
            edge_colors = ['#34a853' if d['label']=='support' else '#ea4335' for _, _, d in G.edges(data=True)]
            nx.draw(G, pos, with_labels=True, node_size=1500, node_color="#f3f3f3", edge_color=edge_colors, font_size=12, font_weight="bold", arrows=True)
            nx.draw_networkx_edge_labels(G, pos, edge_labels={(u,v): d['label'] for u,v,d in G.edges(data=True)}, font_color='gray', font_size=10)
            st.pyplot(plt)
            st.caption(f"{NOTICE_KO}\n\n{NOTICE_EN}")
        except Exception as e:
            st.error(f"{TEXTS['no_network_data'][lang]}: {e}")
    elif network_data and "raw_response" in network_data:
        st.warning(f"{TEXTS['raw_llm']['ko']} / {TEXTS['raw_llm']['en']}:")
        st.code(network_data["raw_response"])
        st.info(f"{TEXTS['no_network_data']['ko']} / {TEXTS['no_network_data']['en']}")
    else:
        st.info(f"{TEXTS['no_network_data']['ko']} / {TEXTS['no_network_data']['en']}")
