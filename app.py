# app.py
# ì—­í• : ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•˜ê³ , ì‚¬ìš©ìž ì¸í„°íŽ˜ì´ìŠ¤ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
# ìµœì¢… ë²„ì „: 'ìš”ì•½ -> ë¶„ì„' 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

import streamlit as st
import pandas as pd
from pyvis.network import Network
import streamlit.components.v1 as components
import google.generativeai as genai

import parsers
import analyzer

# --- TEXTS ë”•ì…”ë„ˆë¦¬ëŠ” ì´ì „ê³¼ ë™ì¼ (ìƒëžµ) ---
TEXTS = {
    "page_title": {"ko": "MirrorOrg MVP", "en": "MirrorOrg MVP"},
    "main_title": {"ko": "ðŸªž MirrorOrg MVP: ì¢…í•© íŒ€ ë¶„ì„", "en": "ðŸªž MirrorOrg MVP: Comprehensive Team Analysis"},
    "main_description": {
        "ko": "'ë¯¸ëŸ¬ì˜¤ì•Œì§€ íŒ€ ë¶„ì„ ì‚¬ë¡€'ì— ê¸°ë°˜í•œ ë‹¤ì°¨ì› í˜‘ì—… ì§„ë‹¨ ë„êµ¬ìž…ë‹ˆë‹¤.\n**íŒ€ ì±„íŒ… ê¸°ë¡(ì¹´ì¹´ì˜¤í†¡, ìŠ¬ëž™ ë“±)**ì„ ì—…ë¡œë“œí•˜ì—¬ íŒ€ í”„ë¡œí•„, í”¼ë¡œë„ ë³€í™”, ê´€ê³„ ë„¤íŠ¸ì›Œí¬ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì§„ë‹¨í•©ë‹ˆë‹¤.",
        "en": "A multi-dimensional collaboration diagnostic tool based on the 'MirrorOrg Team Analysis Case Study'.\nUpload your **team chat history (e.g., KakaoTalk, Slack)** to diagnose Team Profile, Fatigue Trajectory, and Relationship Network."
    },
    "sidebar_header": {"ko": "ì„¤ì •", "en": "Settings"},
    "language_selector": {"ko": "ì–¸ì–´", "en": "Language"},
    "api_key_error_title": {"ko": "API í‚¤ ì„¤ì • ì˜¤ë¥˜", "en": "API Key Configuration Error"},
    "api_key_error_body": {
        "ko": "ì•± ê´€ë¦¬ìžê°€ ì„¤ì •í•œ API í‚¤ì— ë¬¸ì œê°€ ìžˆìŠµë‹ˆë‹¤. Streamlit Cloudì˜ 'Secrets' ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
        "en": "There is an issue with the API key configured by the app administrator. Please check the 'Secrets' settings on Streamlit Cloud."
    },
    "upload_header": {"ko": "1. ì±„íŒ… ê¸°ë¡ ì—…ë¡œë“œ", "en": "1. Upload Chat History"},
    "upload_info": {
        "ko": "íŒ€ ì±„íŒ… ê¸°ë¡ì„ í…ìŠ¤íŠ¸(.txt) íŒŒì¼ë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”. í˜„ìž¬ **ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” í˜•ì‹**ì— ìµœì í™”ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.",
        "en": "Upload your team chat history as a text (.txt) file. Currently optimized for the **KakaoTalk chat format**."
    },
    "file_uploader_label": {"ko": "ë¶„ì„í•  .txt íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", "en": "Choose a .txt file to analyze."},
    "parsing_success": {"ko": "íŒŒì¼ íŒŒì‹± ì„±ê³µ! {count}ê°œì˜ ë©”ì‹œì§€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ({format} í˜•ì‹)", "en": "File parsed successfully! Found {count} messages. (Format: {format})"},
    "parsing_error": {"ko": "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì´ê±°ë‚˜ íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.", "en": "The file format is not supported or the file may be corrupted."},
    "analysis_button": {"ko": "ì¢…í•© ë¶„ì„ ì‹œìž‘í•˜ê¸°", "en": "Start Comprehensive Analysis"},
    "spinner_summary": {"ko": "1/4: ëŒ€í™” ë‚´ìš© ìš”ì•½ ì¤‘...", "en": "1/4: Summarizing chat log..."},
    "spinner_profile": {"ko": "2/4: íŒ€ í”„ë¡œí•„ ë¶„ì„ ì¤‘...", "en": "2/4: Analyzing team profile..."},
    "spinner_timeline": {"ko": "3/4: í”¼ë¡œë„ íƒ€ìž„ë¼ì¸ ë¶„ì„ ì¤‘...", "en": "3/4: Analyzing fatigue timeline..."},
    "spinner_network": {"ko": "4/4: ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘...", "en": "4/4: Analyzing relationship network..."},
    "analysis_complete": {"ko": "âœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", "en": "âœ… Analysis complete!"},
    "file_process_error": {"ko": "íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", "en": "An unknown error occurred while processing the file"},
    "results_header": {"ko": "2. ì§„ë‹¨ ê²°ê³¼", "en": "2. Diagnostic Results"},
    "summary_header": {"ko": "ëŒ€í™”ë¡ ìš”ì•½", "en": "Chat Log Summary"},
    "tab_profile": {"ko": "**íŒ€ í”„ë¡œí•„**", "en": "**Team Profile**"},
    "tab_fatigue": {"ko": "**í”¼ë¡œë„ ë³€í™”**", "en": "**Fatigue Trajectory**"},
    "tab_network": {"ko": "**ê´€ê³„ ë„¤íŠ¸ì›Œí¬**", "en": "**Relationship Network**"},
    "profile_warning": {"ko": "íŒ€ í”„ë¡œí•„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "en": "No team profile data available."},
    "fatigue_warning": {"ko": "í”¼ë¡œë„ íƒ€ìž„ë¼ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "en": "No fatigue timeline data available."},
    "network_warning": {"ko": "ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "en": "Could not generate network data."},
    "raw_response_error": {"ko": "LLMì´ ìœ íš¨í•œ JSONì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•„ëž˜ëŠ” LLMì˜ ì›ë³¸ ì‘ë‹µìž…ë‹ˆë‹¤.", "en": "The LLM did not return valid JSON. Below is the raw response from the LLM."}
}

# --- Page Config & Initialization ---
st.set_page_config(page_title=TEXTS["page_title"]["en"], page_icon="ðŸ¤–", layout="wide")
if 'lang' not in st.session_state: st.session_state.lang = 'ko'
if 'results' not in st.session_state: st.session_state.results = {}

# --- Sidebar ---
with st.sidebar:
    st.header(TEXTS["sidebar_header"][st.session_state.lang])
    lang_choice = st.selectbox(
        label=f'{TEXTS["language_selector"]["en"]} / {TEXTS["language_selector"]["ko"]}',
        options=['í•œêµ­ì–´', 'English'],
        index=0 if st.session_state.lang == 'ko' else 1
    )
    st.session_state.lang = 'ko' if lang_choice == 'í•œêµ­ì–´' else 'en'
lang = st.session_state.lang

# --- API Key Configuration ---
api_configured = False
try:
    genai.configure(api_key=st.secrets["GOOGLE_AI_API_KEY"])
    api_configured = True
except (KeyError, AttributeError):
    st.error(TEXTS["api_key_error_title"][lang])
    st.warning(TEXTS["api_key_error_body"][lang])

# --- UI Rendering Functions ---
def draw_network_graph(network_data, lang):
    # ... (Same as before)
    if not isinstance(network_data, dict) or 'nodes' not in network_data:
        st.warning(TEXTS["network_warning"][lang])
        if isinstance(network_data, str): st.code(network_data)
        return
    # ... (rest of the function is the same)
    net = Network(height="600px", width="100%", bgcolor="#222222", font_color="white", notebook=True, directed=False)
    color_map = {"high_risk": "#FF4136", "medium_risk": "#FF851B", "potential_risk": "#FFDC00", "stable": "#DDDDDD"}
    for node in network_data.get('nodes', []): net.add_node(node.get('id'), label=node.get('label'), size=25)
    for edge in network_data.get('edges', []):
        edge_type = edge.get('type', 'stable')
        net.add_edge(edge.get('from'), edge.get('to'), color=color_map.get(edge_type, "#DDDDDD"), width=4 if edge_type == 'high_risk' else 2)
    try:
        net.save_graph("network_graph.html")
        with open("network_graph.html", 'r', encoding='utf-8') as f: html_content = f.read()
        components.html(html_content, height=620)
    except Exception as e: st.error(f"Error rendering network graph: {e}")

# --- Main App UI ---
st.title(TEXTS["main_title"][lang])
st.markdown(TEXTS["main_description"][lang])

if api_configured:
    st.header(TEXTS["upload_header"][lang])
    st.info(TEXTS["upload_info"][lang])
    uploaded_file = st.file_uploader(TEXTS["file_uploader_label"][lang], type="txt")

    if uploaded_file:
        try:
            file_content = uploaded_file.getvalue().decode("utf-8")
            chat_df = parsers.parse(file_content)

            if isinstance(chat_df, pd.DataFrame):
                detected_format = parsers.detect_format(file_content)
                st.success(TEXTS["parsing_success"][lang].format(count=len(chat_df), format=detected_format))
                
                if st.button(TEXTS["analysis_button"][lang]):
                    st.session_state.results = {} # Clear previous results
                    with st.spinner(TEXTS["spinner_summary"][lang]):
                        summary = analyzer.summarize_chat(chat_df)
                        st.session_state.results['summary'] = summary
                    
                    if isinstance(summary, str) and len(summary) > 10: # Check if summary is valid
                        with st.spinner(TEXTS["spinner_profile"][lang]):
                            st.session_state.results['profile'] = analyzer.analyze_profile(summary)
                        with st.spinner(TEXTS["spinner_timeline"][lang]):
                            st.session_state.results['timeline'] = analyzer.analyze_timeline(summary)
                        with st.spinner(TEXTS["spinner_network"][lang]):
                            st.session_state.results['network'] = analyzer.analyze_network(summary)
                        st.success(TEXTS["analysis_complete"][lang])
                    else:
                        st.error("Failed to generate a valid summary from the chat log.")

            else:
                st.error(TEXTS["parsing_error"][lang])
        except Exception as e:
            st.error(f"{TEXTS['file_process_error'][lang]}: {e}")

    # --- Display Results ---
    if st.session_state.results:
        st.header(TEXTS["results_header"][lang])
        
        with st.expander(TEXTS["summary_header"][lang]):
            st.markdown(st.session_state.results.get('summary', "No summary available."))

        tab1, tab2, tab3 = st.tabs([TEXTS["tab_profile"][lang], TEXTS["tab_fatigue"][lang], TEXTS["tab_network"][lang]])

        with tab1:
            profile_data = st.session_state.results.get('profile')
            if isinstance(profile_data, list):
                st.dataframe(pd.DataFrame(profile_data), use_container_width=True)
            else:
                st.warning(TEXTS["profile_warning"][lang])
                if isinstance(profile_data, str): st.code(profile_data)

        with tab2:
            timeline_data = st.session_state.results.get('timeline')
            if isinstance(timeline_data, dict) and 'error' not in timeline_data:
                try:
                    df = pd.DataFrame.from_dict(timeline_data, orient='index')
                    df.index = pd.to_datetime(df.index).strftime('%Y-%m-%d')
                    st.line_chart(df.sort_index())
                except Exception:
                    st.warning(TEXTS["fatigue_warning"][lang])
                    st.json(timeline_data)
            else:
                st.warning(TEXTS["fatigue_warning"][lang])
                if isinstance(timeline_data, str): st.code(timeline_data)

        with tab3:
            network_data = st.session_state.results.get('network')
            draw_network_graph(network_data, lang)
